---
title: "Characterizing Automobiles"
author: "Hannah Shane Kate Pahama"
date: "03/17/2025"

format: 
  html:  # You will quite likely want to change all but the last one, to taste
    theme:
        light: flatly
        dark: darkly
    mainfont: monospace
    highlight-style: github
    title-block-banner: true
    embed-resources: true

---

# Setup

- Setup

```{r libs}
sh <- suppressPackageStartupMessages
sh(library(tidyverse))
sh(library(caret))
sh(library(fastDummies))
sh(library(class))
sh(library(ISLR)) # for the "Auto" dataframe
```

# Dataframe

- We use the `Auto` dataframe.

```{r df}
head(Auto)
```

- It has the following variable names, which describe various attributes of automobiles.

```{r df2}
names(Auto)
print(Auto$mpg)
```

# Multiple Regression

- Run a linear regression model with `mpg` as the dependent variable and `horsepower` and `year` as features (variables).
- Compute and comment on the RMSE.

```{r regression}
model <- lm(mpg ~ horsepower + year,
            data = Auto)
summary(model)

predicted_mpg <- predict(model,
                         Auto)
rmse <- sqrt(mean((Auto$mpg - predicted_mpg)^2))
print(paste("RMSE:",
            rmse))
```

> <span style="color:red;font-weight:bold">TODO</span>: *Explain*

# Feature Engineering

- Create 10 features based on the `name` column.
- Remove all rows with a missing value.
- Ensure only `mpg` and the engineered features remain.
- Compute and comment on the RMSE.

```{r features}
anyNA(Auto)
str(Auto)

Auto$name <- as.character(Auto$name)

Auto <- Auto %>%
  mutate(
    brand = word(name, 1),
    is_wagon = ifelse(str_detect(name,
                                 "convertible"),
                      1, 0),
    name_length = nchar(name),
    word_count = str_count(name,
                           "\\S+"),
    is_sporty = ifelse(str_detect(name, "sport|fast|speed"),
                       1, 0)
  )

#we skip na dropping since there exist no na in anyna

Auto <- dummy_cols(Auto,
                   select_columns = "brand",
                   remove_first_dummy = TRUE)

model <- lm(
  mpg ~ .,
  data = Auto
)

summary(model)
```

> <span style="color:red;font-weight:bold">TODO</span>: *The regression results indicate a significant and well-fitting model(?), with 98.16% of the variation in the dependent variable explained by the predictors (Multiple R-squared) and a strong overall significance (p-value < 2.2e-16). However, the residual standard error of 2.272 suggests some variability remains, which could be explored further with additional diagnostics or feature refinement.*

# Classification

- Use either of $K$-NN or Naive Bayes to predict whether an automobile is a `chevrolet` or a `honda`.
- Explain your choice of technique.
- Report on your Kappa value.

```{r classification}
Auto_class <- Auto %>%
  filter(brand %in% c("chevrolet",
                      "honda")) %>%
  mutate(target = ifelse(brand == "chevrolet",
                         1, 0)) %>%
  select(-brand, -name)

set.seed(123)

# Split data
train_index <- createDataPartition(Auto_class$target,
                                   p = 0.8,
                                   list = FALSE)
train_data <- Auto_class[train_index, ]
test_data <- Auto_class[-train_index, ]

# Preprocess: Normalize numeric features (center and scale)
pre_proc <- preProcess(train_data[, -ncol(train_data)], 
                       method = c("center",
                                  "scale"))
train_data_scaled <- predict(pre_proc,
                             train_data)
test_data_scaled <- predict(pre_proc,
                            test_data)

train_data_scaled$target <- factor(train_data$target,
                                   levels = c(0, 1))
test_data_scaled$target <- factor(test_data$target,
                                  levels = c(0, 1))

# Train KNN mod
knn_model <- train(target ~ ., 
                   data = train_data_scaled, 
                   method = "knn", 
                   tuneLength = 5)
print(knn_model)

# Generate predictions
predictions <- predict(knn_model,
                       test_data_scaled)

predictions <- factor(predictions,
                      levels = levels(test_data_scaled$target))

# Confusion Mat
confusion_mat <- confusionMatrix(predictions,
                                 test_data_scaled$target)
print(confusion_mat)

# Calculate Kappa
kappa_value <- confusion_mat$overall["Kappa"]
print(paste("Kappa Value:",
            kappa_value))
```

> <span style="color:red;font-weight:bold">TODO</span>: *The model achieved perfect performance with 100% accuracy, sensitivity, specificity, and a Kappa value of 1, indicating flawless agreement between predictions and actual results. However, the small dataset and potential class imbalance could limit generalizability.*

# Binary Classification

- Predict whether a car is a `honda`.
- Use model weights.
- Display and comment on an ROC curve.

```{r binary classification}
library(tidyverse)
library(caret)
library(pROC)

Auto_class <- Auto %>%
  mutate(target = ifelse(brand == "honda",
                         1, 0)) %>%
  select(-brand,
         -name)

# Train/Test Split
set.seed(123)
train_index <- createDataPartition(Auto_class$target,
                                   p = 0.8,
                                   list = FALSE)
train_data <- Auto_class[train_index, ]
test_data <- Auto_class[-train_index, ]

#Preprocessing
pre_proc <- preProcess(train_data[, -ncol(train_data)],
                       method = c("center", "scale"))
train_data_scaled <- predict(pre_proc,
                             train_data)
test_data_scaled <- predict(pre_proc,
                            test_data)

# Convert 'target' to factor
train_data_scaled$target <- factor(train_data$target,
                                   levels = c(0, 1))
test_data_scaled$target <- factor(test_data$target,
                                  levels = c(0, 1))

# Logistic Regression Mod
logistic_model <- glm(target ~ .,
                      data = train_data_scaled,
                      family = binomial)
summary(logistic_model)

# Make Pred
test_data_scaled$predicted_probs <- predict(logistic_model,
                                            test_data_scaled,
                                            type = "response")

# Step 6: ROC Curve
roc_curve <- roc(test_data_scaled$target,
                 test_data_scaled$predicted_probs)

# Plot ROC
plot(roc_curve,
     col = "orange",
     main = "ROC Curve for Predicting Honda")
abline(a = 0,
       b = 1,
       col = "deeppink",
       lty = 2)

# Calculate and Display AUC
auc_value <- auc(roc_curve)
print(paste("AUC:",
            auc_value))
```

> <span style="color:red;font-weight:bold">TODO</span>: *Running out of time*

# Ethics

- Based on your analysis, comment on the [Clean Air Act of 1970 and Ammendments of 1977](https://www.epa.gov/clean-air-act-overview/evolution-clean-air-act)
- Discuss the civic reposibilities of data scientists for:
    - Big Data and Human-Centered Computing
    - Democratic Institutions
    - Climate Change
- Provide at least one statistical measure for each, such as a RMSE, Kappa value, or ROC curve.

> <span style="color:red;font-weight:bold">TODO</span>: Big Data and Human-Centered Computing

```{r big data}
# Your code here
```

> <span style="color:red;font-weight:bold">TODO</span>: Democratic Institutions

```{r democracy}
# Your code here
```

> <span style="color:red;font-weight:bold">TODO</span>: Climate Change

```{r climate}
# Your code here
```
